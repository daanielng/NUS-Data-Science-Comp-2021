# -*- coding: utf-8 -*-
"""Copy of Team 31_script

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Y_u66R3t8Di36Lw5sbbK5ERscunbhfG
"""

# import libraries
import cv2
import numpy as np
import matplotlib.pyplot as plt

import os
import re
from os.path import isfile, join

# declare path for frames
train_frames = os.listdir("/content/drive/MyDrive/Team 31 DSC 2021 submission/train_data/train_images")
train_frames

# sort file names
train_frames.sort(key=lambda f: int(re.sub('\D', '', f)))
train_frames

# create empty list to store images
train_images = []

for frame in train_frames:
  # read the frames, concatenate the strings
  temp_img = cv2.imread("/content/drive/MyDrive/Team 31 DSC 2021 submission/train_data/train_images/" + frame)
  # append the temp image into the list of images
  train_images.append(temp_img)

#print(train_images)

#print(train_images[0])

train_images[0].shape

plt.imshow(train_images[0])

train_RGB = []
for i in train_images:
  img_RGB = cv2.cvtColor(i, cv2.COLOR_BGR2RGB)
  train_RGB.append(img_RGB)

plt.imshow(train_RGB[0])

train_images = train_RGB
plt.imshow(train_images[0])

# the images have different dimensions so we want to resize and rescale all images into the same dimensions
heights = []
for i in train_images:
  heights.append(i.shape[0])

print("Heights:", heights)

widths = []
for i in train_images:
  widths.append(i.shape[1])

print("Widths:", widths)

aspectRatios = []
#print(len(heights), len(widths)) # 116 116
for i in range(len(heights)):
  aspectRatios.append(widths[i]/heights[i])

print("Aspect Ratios:", aspectRatios)
print("Average Aspect Ratio:", np.mean(aspectRatios))

print("Min. height:", min(heights))
print("Min. width:", min(widths))

# 4:5
# 320:400

train_resized = []
# Resizing
for i in train_images:
  resized = cv2.resize(i, (320,400))
  train_resized.append(resized)

plt.imshow(train_resized[12])

train_images = train_resized
plt.imshow(train_images[12])

# bilat = []
# for i in train_images:
#   img_bilat = cv2.bilateralFilter(i, 35, 75, 75)
#   bilat.append(img_bilat)

# plt.imshow(bilat[0])

# train_images = bilat
# plt.imshow(train_images[12])

# gray = []
# for i in train_images:
#   img_gray = cv2.cvtColor(i, cv2.COLOR_RGB2GRAY)
#   gray.append(img_gray)

# plt.imshow(gray[0], cmap="Greys_r")

# convert to hsv
train_hsv = []
for i in train_images:
  temp = cv2.cvtColor(i, cv2.COLOR_RGB2HSV)
  train_hsv.append(temp)

plt.imshow(train_hsv[0])

train_thresh = []
for i in train_hsv:
  ret, thresh = cv2.threshold(i, 30, 255, cv2.THRESH_BINARY)
  train_thresh.append(thresh)

plt.imshow(train_thresh[0])

train_dilate = []
kernel = np.ones((4,4),np.uint8)
for i in train_thresh:
  dilated = cv2.dilate(i,kernel,iterations = 1)
  train_dilate.append(dilated)

plt.imshow(train_dilate[0])

# Canny edge detector
train_canny = []
for i in train_dilate:
  temp = cv2.Canny(i, 20, 30)
  train_canny.append(temp)

plt.imshow(train_canny[0])
# shape before canny (400, 320, 3)
# shape after canny (400, 320)

contours, hierarchy = cv2.findContours(train_canny[0], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
#print(contours)
#print(len(contours)) # 662

# cntrAreas = []
# for i,cntr in enumerate(contours):
#   cntrAreas.append(cv2.contourArea(cntr))

# print(cntrAreas) # 662
# print(len(cntrAreas)) # 662
# print(min(cntrAreas), max(cntrAreas), np.mean(cntrAreas)) # 0.0 48.0 1.0717522658610272

# valid_cntrs = []
# for i,cntr in enumerate(contours):
#   x,y,w,h = cv2.boundingRect(cntr)
#   if (cv2.contourArea(cntr) > np.mean(cntrAreas)):
#     valid_cntrs.append(cntr)

# count of discovered contours
# print(len(valid_cntrs))

cpy = train_images[0].copy()
#plt.imshow(cpy)
cv2.drawContours(cpy, contours, -1, (0, 255, 0), 2)
plt.imshow(cpy)

print(contours[0])
print(cv2.boundingRect(contours[0]))

# convert to hsv
train_hsv = []
for i in train_images:
  temp = cv2.cvtColor(i, cv2.COLOR_RGB2HSV)
  train_hsv.append(temp)

#plt.imshow(train_hsv[0])

# mask of madness (5,0,25) ~ (55, 255, 230)
lowerBound = np.array([25, 0, 25], dtype = "uint8")
upperBound = np.array([120, 255, 230], dtype = "uint8") 
mask = cv2.inRange(train_hsv[0], lowerBound, upperBound)

output = cv2.bitwise_and(train_hsv[0], train_hsv[0], mask=mask)

plt.imshow(output)

import pandas as pd
train_df = pd.read_csv('/content/drive/MyDrive/Team 31 DSC 2021 submission/train_data/train_labels.csv', index_col=None)
#train_df.head()
objCount = train_df["object_count_gt"]
#objCount
train_labels = objCount.tolist()
#train_labels
#print(len(train_labels)) # 116

from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
train_labels

#!pip install -U efficientnet
!pip install keras_efficientnets

import keras
from keras_efficientnets import EfficientNetB0

new_input = keras.Input(shape=(400, 320, 3))
base_model = EfficientNetB0(weights="imagenet", include_top=False, input_tensor=new_input)
print(base_model.summary())

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical 

model = Sequential()
model.add(base_model)
base_model.trainable = False
#model.add(Dense(units = 120, activation='relu'))
#model.add(Dense(units = 120, activation = 'relu'))
model.add(Dense(units = 1, activation='relu'))
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])
#model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))
model.fit(x=train_images, y=train_labels, batch_size=32, epochs=10, validation_split=0.2)